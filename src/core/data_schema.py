# src/core/data_schema.py
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional, Literal
from enum import Enum # <-- Make sure to import Enum

# --- Models to describe input data schema ---
# Pydantic uses classes, but they are mainly for data validation/typing.
# We consider this an acceptable "exception" since it's an external library.
class ColumnSchema(BaseModel):
    name: str = Field(..., description="Column name.")
    dtype: str = Field(..., description="Inferred data type (e.g., 'object', 'int64', 'float64', 'datetime64[ns]').")
    is_nullable: bool = Field(..., description="Indicates if the column can contain null values.")
    unique_values: Optional[int] = Field(None, description="Number of unique values (if applicable).")
    top_values: Optional[List[Dict[str, Any]]] = Field(None, description="Most frequent values and their counts.")
    min_value: Optional[Any] = Field(None, description="Minimum value (if numeric/datetime).")
    max_value: Optional[Any] = Field(None, description="Maximum value (if numeric/datetime).")

class DataFrameSchema(BaseModel):
    num_rows: int = Field(..., description="Total number of rows in the DataFrame.")
    num_columns: int = Field(..., description="Total number of columns in the DataFrame.")
    columns: List[ColumnSchema] = Field(..., description="List of schemas for each column.")
    overview_summary: str = Field(..., description="A textual summary generated by the LLM about the dataset.")
    load_info: Optional[Dict[str, Any]] = Field(None, description="Information about how the data was loaded, including warnings and method used.")

# --- Define OperationType Enum for Transformation Steps ---
class OperationType(str, Enum):
    """Defines the allowed types of data transformation operations."""
    STANDARDIZE_COLUMN_NAME = "standardize_column_name"
    HANDLE_NULLS = "handle_nulls"
    CONVERT_CASE = "convert_case"
    CHANGE_DATATYPE = "change_datatype"
    FILTER_ROWS = "filter_rows"
    AGGREGATE = "aggregate"
    REMOVE_DUPLICATES = "remove_duplicates"
    DERIVE_COLUMN = "derive_column"
    RENAME_COLUMN = "rename_column"
    CONVERT_TO_DATETIME = "convert_to_datetime"
    CONVERT_TO_NUMERIC = "convert_to_numeric"
    FILL_NULLS = "fill_nulls"
    # --- NEW OPERATIONS ---
    NORMALIZE = "normalize" # For numeric data normalization operations
    FEATURE_ENGINEERING = "feature_engineering" # For creating new features
    DATA_VISUALIZATION = "data_visualization" # Although not a 'transformation' of data, if the LLM insists, we can allow and treat it as a 'step' in the overall plan. However, ideally it would be a separate agent.
                                              # If it's only for the plan, it can be a placeholder for reports/dashboards.

# --- Models for Transformation Plan ---
class TransformationStep(BaseModel):
    operation_type: OperationType = Field(..., description="Type of transformation operation.")
    column_name: Optional[str] = Field(None, description="Target column name for the transformation (if applicable).")
    params: Dict[str, Any] = Field({}, description="Specific parameters for the operation (e.g., {'value': 0} to fill nulls, {'case': 'upper'} to convert case).")
    justification: str = Field(..., description="Justification for this transformation based on instructions and analysis.")
    expected_outcome: str = Field(..., description="Expected result after applying this transformation.")


class TransformationPlan(BaseModel):
    # CHANGE THIS LINE: from DataFrameSchema to str
    initial_data_overview_summary: str = Field(..., description="A textual summary of the initial data, derived from the DataFrameSchema's overview_summary.")
    transformation_steps: List[TransformationStep] = Field(..., description="Ordered list of transformation steps to be applied.")
    final_output_format: Literal["parquet"] = "parquet"
    overall_summary: str = Field(..., description="A general summary of what the pipeline will do.")
    requires_confirmation: bool = Field(True, description="Indicates if the plan requires user confirmation before generating code.")

# --- Model for Approval Agent Response ---
class UserApproval(BaseModel):
    approved: bool = Field(..., description="True if the user approved the plan, False otherwise.")
    feedback: Optional[str] = Field(None, description="Optional user feedback if the plan is not approved or for refinement.")

# --- Graph State (TypedDicts are preferred over large classes for state in functional) ---
from typing import TypedDict
class GraphState(TypedDict):
    """
    Represents the graph state in Langgraph.
    The fields are the information that flows between nodes.
    """
    file_path: str
    source_type: Literal["csv", "json", "excel"]
    df_schema: Optional[DataFrameSchema] # Output from DataIngestion
    user_instructions: Optional[str] # Input for TransformationPlanner (now optional/general)
    transformation_plan: Optional[TransformationPlan] # Output from TransformationPlanner
    user_approval: Optional[UserApproval] # Output from ApprovalAgent
    generated_code: Optional[str] # Output from CodeGenerationAgent
    test_results: Optional[Dict[str, Any]] # Output from TestAgent
    code_quality_result: Optional[Dict[str, Any]] # Output from CodeQualityAgent
    refactored_code_result: Optional[Dict[str, Any]] # Output from RefactoringPipelineAgent
    final_quality_result: Optional[Dict[str, Any]] # Output from FinalQualityCheckAgent
    # Add other fields as needed for the flow

class SearchQueries(BaseModel): # Make sure this is also present
    """Represents a list of search queries."""
    queries: List[str] = Field(
        ...,
        description="A list of concise search queries relevant to data transformation and cleaning."
    )