"""
PydanticAI Agent for code quality analysis and Python functional best practices
Integration with pipeline generated by LangGraph
"""

import ast
import re
import os
from pathlib import Path
from typing import List, Dict, Any, Optional
from enum import Enum
from pydantic import BaseModel, Field
from pydantic_ai import Agent
from pydantic_ai.models.groq import GroqModel
from pydantic_ai.providers.groq import GroqProvider

class QualityLevel(str, Enum):
    EXCELLENT = "excellent"
    GOOD = "good"
    NEEDS_IMPROVEMENT = "needs_improvement"
    POOR = "poor"

class FunctionalParadigmIssue(BaseModel):
    """Problem found related to the functional paradigm"""
    line_number: int
    issue_type: str
    description: str
    suggestion: str
    severity: str = Field(..., pattern="^(low|medium|high|critical)$")

class CodeMetrics(BaseModel):
    """Basic code metrics"""
    total_lines: int
    total_functions: int
    average_function_length: float
    cyclomatic_complexity_avg: float
    pure_functions_count: int
    impure_functions_count: int

class FunctionalPracticesReport(BaseModel):
    """Complete functional practices analysis report"""
    file_analyzed: str
    overall_quality: QualityLevel
    functional_score: float = Field(ge=0, le=100, description="Score 0-100 for functional practices")
    
    # Metrics
    code_metrics: CodeMetrics
    
    # Problems found
    functional_issues: List[FunctionalParadigmIssue]
    
    # Specific analyses
    immutability_violations: List[str]
    side_effects_detected: List[str]
    pure_function_opportunities: List[str]
    
    # Recommendations
    immediate_fixes: List[str]
    refactoring_suggestions: List[str]
    best_practices_recommendations: List[str]
    
    # Executive summary
    summary: str
    next_actions: List[str]

class CodeAnalysisConfig(BaseModel):
    """Configuration for code analysis"""
    file_path: str
    check_immutability: bool = True
    check_pure_functions: bool = True
    check_side_effects: bool = True
    min_function_length: int = 5
    max_function_length: int = 20

class CodeQualityResult(BaseModel):
    """Result of code quality analysis for integration with workflow"""
    analysis_report: FunctionalPracticesReport
    quality_score: float
    recommendations: List[str]
    critical_issues: List[str]
    improvement_suggestions: List[str]
    is_acceptable: bool = Field(..., description="Whether the code quality meets minimum standards")

# Create the specialized agent
functional_code_analyzer = Agent(
    model=GroqModel(
        model_name="llama-3.1-8b-instant",
        provider=GroqProvider(api_key=os.getenv("GROQ_API_KEY"))
    ),
    deps_type=CodeAnalysisConfig,
    result_type=FunctionalPracticesReport,
    system_prompt="""
    You are a senior software engineer with expertise in Python functional programming and Data Engineering Pipelines.
    
    Your mission is to analyze Python code generated automatically and evaluate:
    
    FUNCTIONAL PARADIGM:
    - Pure functions (no side effects)
    - Immutable data
    - Avoid mutable global state
    - Use map, filter, reduce when appropriate
    - Function composition
    - Avoid loops when possible
    
    PYTHON BEST PRACTICES:
    - Small and focused functions (single responsibility)
    - Descriptive names
    - Type hints
    - Adequate docstrings
    - Low cyclomatic complexity
    
    CODE QUALITY:
    - Readability
    - Maintainability  
    - Testability
    - Performance
    
    Be rigorous but constructive. Provide specific and actionable suggestions.
    Consider that the code was generated by AI and may have repetitive patterns.
    
    IMPORTANT: Use the provided tools to analyze the code. Do not attempt to call tools that are not available.
    """
)

@functional_code_analyzer.tool
async def read_python_file(ctx) -> str:
    """Read the Python file to be analyzed"""
    try:
        file_path = Path(ctx.deps.file_path)
        if not file_path.exists():
            return f"ERROR: File {file_path} not found"
        
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        return f"File read successfully. Size: {len(content)} characters\n\nContent:\n{content}"
    
    except Exception as e:
        return f"ERROR reading file: {str(e)}"

@functional_code_analyzer.tool
async def analyze_ast_structure(ctx) -> str:
    """Analyze the AST structure of the code for detailed metrics"""
    try:
        file_path = Path(ctx.deps.file_path)
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        tree = ast.parse(content)
        
        # Basic analysis
        functions = [node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
        classes = [node for node in ast.walk(tree) if isinstance(node, ast.ClassDef)]
        imports = [node for node in ast.walk(tree) if isinstance(node, (ast.Import, ast.ImportFrom))]
        
        # Complexity analysis
        function_info = []
        for func in functions:
            func_lines = func.end_lineno - func.lineno if func.end_lineno else 0
            
            # Potential side effects detection
            has_global = any(isinstance(node, ast.Global) for node in ast.walk(func))
            has_nonlocal = any(isinstance(node, ast.Nonlocal) for node in ast.walk(func))
            has_print = any(isinstance(node, ast.Call) and 
                          isinstance(node.func, ast.Name) and 
                          node.func.id == 'print' for node in ast.walk(func))
            
            # Mutations detection
            mutations = len([node for node in ast.walk(func) 
                           if isinstance(node, (ast.AugAssign, ast.AnnAssign)) or
                           (isinstance(node, ast.Assign) and 
                            any(isinstance(target, ast.Subscript) for target in node.targets))])
            
            function_info.append({
                'name': func.name,
                'lines': func_lines,
                'has_global': has_global,
                'has_nonlocal': has_nonlocal,
                'has_print': has_print,
                'mutations': mutations,
                'lineno': func.lineno
            })
        
        analysis = {
            'total_lines': len(content.split('\n')),
            'functions_count': len(functions),
            'classes_count': len(classes),
            'imports_count': len(imports),
            'function_details': function_info
        }
        
        return f"Complete AST analysis:\n{analysis}"
    
    except Exception as e:
        return f"ERROR in AST analysis: {str(e)}"

@functional_code_analyzer.tool
async def detect_functional_patterns(ctx) -> str:
    """Detect functional specific patterns in the code"""
    try:
        file_path = Path(ctx.deps.file_path)
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Functional patterns to detect
        patterns = {
            'map_usage': len(re.findall(r'\bmap\s*\(', content)),
            'filter_usage': len(re.findall(r'\bfilter\s*\(', content)),
            'reduce_usage': len(re.findall(r'\breduce\s*\(', content)),
            'lambda_usage': len(re.findall(r'\blambda\b', content)),
            'list_comprehensions': len(re.findall(r'\[.*for.*in.*\]', content)),
            'dict_comprehensions': len(re.findall(r'\{.*for.*in.*\}', content)),
            'generator_expressions': len(re.findall(r'\(.*for.*in.*\)', content)),
            'yield_usage': len(re.findall(r'\byield\b', content)),
            'partial_usage': len(re.findall(r'\bpartial\s*\(', content)),
            'chain_usage': len(re.findall(r'\bchain\s*\(', content))
        }
        
        # Anti-functional patterns
        anti_patterns = {
            'global_variables': len(re.findall(r'\bglobal\s+\w+', content)),
            'mutable_defaults': len(re.findall(r'def\s+\w+\([^)]*=\s*\[\]', content)) + 
                              len(re.findall(r'def\s+\w+\([^)]*=\s*\{\}', content)),
            'print_statements': len(re.findall(r'\bprint\s*\(', content)),
            'input_statements': len(re.findall(r'\binput\s*\(', content)),
            'file_operations': len(re.findall(r'\bopen\s*\(', content)),
            'random_usage': len(re.findall(r'\brandom\.', content))
        }
        
        return f"Functional patterns detected:\n{patterns}\n\nAnti-functional patterns detected:\n{anti_patterns}"
    
    except Exception as e:
        return f"ERROR in functional patterns detection: {str(e)}"

@functional_code_analyzer.tool
async def check_pandas_functional_usage(ctx) -> str:
    """Check functional specific usage of Pandas"""
    try:
        file_path = Path(ctx.deps.file_path)
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Functional patterns of Pandas
        pandas_patterns = {
            'method_chaining': len(re.findall(r'\..*\n\s*\.', content, re.MULTILINE)),
            'pipe_usage': len(re.findall(r'\.pipe\s*\(', content)),
            'apply_usage': len(re.findall(r'\.apply\s*\(', content)),
            'transform_usage': len(re.findall(r'\.transform\s*\(', content)),
            'agg_usage': len(re.findall(r'\.agg\s*\(', content)),
            'query_usage': len(re.findall(r'\.query\s*\(', content)),
            'assign_usage': len(re.findall(r'\.assign\s*\(', content)),
            'groupby_chains': len(re.findall(r'\.groupby\([^)]*\)\s*\.', content))
        }
        
        # Anti-patterns of Pandas
        pandas_anti_patterns = {
            'iterrows_usage': len(re.findall(r'\.iterrows\s*\(\s*\)', content)),
            'itertuples_usage': len(re.findall(r'\.itertuples\s*\(\s*\)', content)),
            'loop_over_dataframe': len(re.findall(r'for.*in\s+\w+\.index', content)),
            'inplace_operations': len(re.findall(r'inplace\s*=\s*True', content)),
            'chained_indexing': len(re.findall(r'\[\w+\]\[\w+\]', content))
        }
        
        return f"Functional Pandas patterns:\n{pandas_patterns}\n\nAnti-Pandas patterns:\n{pandas_anti_patterns}"
    
    except Exception as e:
        return f"ERROR in Pandas analysis: {str(e)}"

async def analyze_generated_pipeline(file_path: str, config: Optional[Dict[str, Any]] = None) -> FunctionalPracticesReport:
    """
    Main function to analyze the generated pipeline
    
    Args:
        file_path: Path to the generated Python file
        config: Optional configurations for analysis
    
    Returns:
        Complete functional analysis report
    """
    
    # Default configuration
    analysis_config = CodeAnalysisConfig(
        file_path=file_path,
        check_immutability=config.get('check_immutability', True) if config else True,
        check_pure_functions=config.get('check_pure_functions', True) if config else True,
        check_side_effects=config.get('check_side_effects', True) if config else True,
        min_function_length=config.get('min_function_length', 5) if config else 5,
        max_function_length=config.get('max_function_length', 20) if config else 20
    )
    
    # Execute analysis
    result = await functional_code_analyzer.run(
        f"""
        Analyze the Python file generated by the pipeline: {file_path}
        
        Focus especially on:
        1. Adequate use of functional practices
        2. Quality of the generated Pandas code
        3. Detection of unnecessary side effects
        4. Opportunities for functional improvements
        5. Conformity with Python best practices
        
        Provide a detailed and actionable report.
        """,
        deps=analysis_config
    )
    
    return result.data

def log_analysis_results(report: FunctionalPracticesReport) -> None:
    """Log the analysis results for debugging"""
    print(f"\n🔍 CODE QUALITY ANALYSIS - {report.file_analyzed}")
    print(f"📊 Functional Score: {report.functional_score}/100")
    print(f"⭐ Overall Quality: {report.overall_quality}")
    print(f"🔧 Found Problems: {len(report.functional_issues)}")
    print(f"📝 Immediate Recommendations: {len(report.immediate_fixes)}")
    print("-" * 50)

# --- INTEGRATION WITH LANGGRAPH WORKFLOW ---

def code_quality_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    LangGraph node for code quality analysis
    
    This node analyzes the generated code for functional programming practices
    and code quality standards.
    """
    print("\n🔍 Starting Code Quality Analysis...")
    
    # Get the generated code file path from the state
    generated_code = state.get("generated_code")
    latest_code_path = state.get("latest_code_path")
    
    if not generated_code:
        print("❌ No generated code found in state")
        return state
    
    # Use the latest code path if available, otherwise try to determine it
    if latest_code_path:
        output_file = latest_code_path
    else:
        # Fallback: try to determine the file path
        file_path = state.get("file_path", "data/input.csv")
        base_filename = os.path.splitext(os.path.basename(file_path))[0]
        output_file = f"pipelines/generated/pipeline_{base_filename}_latest.py"
    
    try:
        # Use the simplified analysis function
        analysis_report = analyze_code_quality_simple(output_file)
        
        # Create quality result
        critical_issues = [
            issue.description for issue in analysis_report.functional_issues 
            if issue.severity in ["high", "critical"]
        ]
        
        quality_result = CodeQualityResult(
            analysis_report=analysis_report,
            quality_score=analysis_report.functional_score,
            recommendations=analysis_report.immediate_fixes,
            critical_issues=critical_issues,
            improvement_suggestions=analysis_report.refactoring_suggestions,
            is_acceptable=analysis_report.functional_score >= 70  # Minimum acceptable score
        )
        
        # Log results
        log_analysis_results(analysis_report)
        
        # Update state
        state["code_quality_result"] = quality_result
        
        if quality_result.is_acceptable:
            print("✅ Code quality analysis completed - Code meets quality standards")
        else:
            print("⚠️ Code quality analysis completed - Code needs improvements")
            print(f"Critical issues found: {len(critical_issues)}")
        
        return state
        
    except Exception as e:
        print(f"❌ Error in code quality analysis: {e}")
        # Create a basic error result
        error_report = FunctionalPracticesReport(
            file_analyzed=output_file,
            overall_quality=QualityLevel.POOR,
            functional_score=0.0,
            code_metrics=CodeMetrics(
                total_lines=0,
                total_functions=0,
                average_function_length=0.0,
                cyclomatic_complexity_avg=0.0,
                pure_functions_count=0,
                impure_functions_count=0
            ),
            functional_issues=[],
            immutability_violations=[],
            side_effects_detected=[],
            pure_function_opportunities=[],
            immediate_fixes=[f"Error in analysis: {str(e)}"],
            refactoring_suggestions=[],
            best_practices_recommendations=[],
            summary=f"Analysis failed: {str(e)}",
            next_actions=["Review the generated code manually"]
        )
        
        error_result = CodeQualityResult(
            analysis_report=error_report,
            quality_score=0.0,
            recommendations=[f"Analysis error: {str(e)}"],
            critical_issues=[f"Analysis failed: {str(e)}"],
            improvement_suggestions=[],
            is_acceptable=False
        )
        
        state["code_quality_result"] = error_result
        return state

# --- SIMPLIFIED CODE QUALITY ANALYSIS ---
# SUBSTITUA A FUNÇÃO analyze_code_quality_simple POR ESTA VERSÃO CORRIGIDA:

def analyze_code_quality_simple(file_path: str) -> FunctionalPracticesReport:
    """
    Improved code quality analysis with corrected scoring
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Basic metrics
        lines = content.split('\n')
        total_lines = len(lines)
        
        # Parse AST for analysis
        tree = ast.parse(content)
        functions = [node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
        classes = [node for node in ast.walk(tree) if isinstance(node, ast.ClassDef)]
        
        # Calculate metrics
        total_functions = len(functions)
        function_lengths = []
        pure_functions = 0
        impure_functions = 0
        functional_issues = []
        
        for func in functions:
            func_lines = func.end_lineno - func.lineno if func.end_lineno else 0
            function_lengths.append(func_lines)
            
            # Check for side effects
            has_global = any(isinstance(node, ast.Global) for node in ast.walk(func))
            has_print = any(isinstance(node, ast.Call) and 
                          isinstance(node.func, ast.Name) and 
                          node.func.id == 'print' for node in ast.walk(func))
            has_file_ops = any(isinstance(node, ast.Call) and 
                             isinstance(node.func, ast.Name) and 
                             node.func.id in ['open', 'write', 'read'] for node in ast.walk(func))
            
            if has_global or has_file_ops:
                impure_functions += 1
                functional_issues.append(FunctionalParadigmIssue(
                    line_number=func.lineno,
                    issue_type="side_effect",
                    description=f"Function '{func.name}' has side effects",
                    suggestion="Consider making this function pure by removing side effects",
                    severity="medium"
                ))
            else:
                pure_functions += 1
        
        # Calculate averages
        avg_function_length = sum(function_lengths) / len(function_lengths) if function_lengths else 0
        
        # ===== CORRECTED SCORING SYSTEM =====
        functional_score = 0
        
        if total_functions > 0:
            # 1. Base code structure (30 points)
            if total_functions >= 3:
                functional_score += 15  # Has multiple functions
            if avg_function_length <= 20:
                functional_score += 10  # Good function size
            if len(functional_issues) == 0:
                functional_score += 5   # No major issues
            
            # 2. Functional patterns (30 points)
            map_usage = len(re.findall(r'\bmap\s*\(', content))
            filter_usage = len(re.findall(r'\bfilter\s*\(', content))
            list_comprehensions = len(re.findall(r'\[.*for.*in.*\]', content))
            lambda_usage = len(re.findall(r'\blambda\b', content))
            
            if map_usage > 0 or filter_usage > 0:
                functional_score += 10
            if list_comprehensions > 0:
                functional_score += 10
            if lambda_usage > 0:
                functional_score += 5
            if pure_functions / total_functions >= 0.7:
                functional_score += 5  # Most functions are pure
            
            # 3. Pandas best practices (25 points)
            method_chaining = len(re.findall(r'\..*\n\s*\.', content, re.MULTILINE))
            pipe_usage = len(re.findall(r'\.pipe\s*\(', content))
            apply_usage = len(re.findall(r'\.apply\s*\(', content))
            
            if method_chaining > 2:
                functional_score += 10  # Good chaining
            if pipe_usage > 0:
                functional_score += 5
            if apply_usage > 0:
                functional_score += 5
            
            # No iterrows/itertuples (good practice)
            iterrows_usage = len(re.findall(r'\.iterrows\s*\(\s*\)', content))
            itertuples_usage = len(re.findall(r'\.itertuples\s*\(\s*\)', content))
            if iterrows_usage == 0 and itertuples_usage == 0:
                functional_score += 5
            
            # 4. Code quality (15 points)
            type_hints = len(re.findall(r':\s*\w+', content))
            docstrings = len(re.findall(r'""".*?"""', content, re.DOTALL))
            
            if type_hints > 0:
                functional_score += 5
            if docstrings > 0:
                functional_score += 5
            if total_lines > 50:  # Substantial code
                functional_score += 5
            
            # 5. Penalties (subtract points for bad practices)
            global_vars = len(re.findall(r'\bglobal\s+\w+', content))
            print_statements = len(re.findall(r'\bprint\s*\(', content))
            inplace_operations = len(re.findall(r'inplace\s*=\s*True', content))
            
            if global_vars > 0:
                functional_score -= 5
            if print_statements > 3:
                functional_score -= 5
            if inplace_operations > 0:
                functional_score -= 5
            
            # Ensure score is within bounds
            functional_score = max(0, min(100, functional_score))
        
        else:
            # No functions found - very basic score
            if total_lines > 20:
                functional_score = 25  # Has some code
            else:
                functional_score = 10  # Very minimal
        
        # Determine quality level based on corrected scoring
        if functional_score >= 85:
            quality_level = QualityLevel.EXCELLENT
        elif functional_score >= 70:
            quality_level = QualityLevel.GOOD
        elif functional_score >= 50:
            quality_level = QualityLevel.NEEDS_IMPROVEMENT
        else:
            quality_level = QualityLevel.POOR
        
        # Generate smart recommendations
        immediate_fixes = []
        refactoring_suggestions = []
        
        if functional_score < 70:
            if len(re.findall(r'\bprint\s*\(', content)) > 3:
                immediate_fixes.append("Replace print statements with proper logging")
            if len(re.findall(r'\bglobal\s+\w+', content)) > 0:
                immediate_fixes.append("Remove global variable usage")
            if avg_function_length > 25:
                immediate_fixes.append("Break down large functions into smaller ones")
        
        if functional_score < 85:
            if len(re.findall(r':\s*\w+', content)) == 0:
                refactoring_suggestions.append("Add type hints to functions")
            if len(re.findall(r'\.pipe\s*\(', content)) == 0:
                refactoring_suggestions.append("Use .pipe() for better method chaining")
            if len(re.findall(r'\[.*for.*in.*\]', content)) == 0:
                refactoring_suggestions.append("Use list comprehensions where appropriate")
        
        # Create the report
        report = FunctionalPracticesReport(
            file_analyzed=file_path,
            overall_quality=quality_level,
            functional_score=functional_score,
            code_metrics=CodeMetrics(
                total_lines=total_lines,
                total_functions=total_functions,
                average_function_length=avg_function_length,
                cyclomatic_complexity_avg=1.0,  # Simplified
                pure_functions_count=pure_functions,
                impure_functions_count=impure_functions
            ),
            functional_issues=functional_issues,
            immutability_violations=[],
            side_effects_detected=[],
            pure_function_opportunities=[],
            immediate_fixes=immediate_fixes,
            refactoring_suggestions=refactoring_suggestions,
            best_practices_recommendations=[
                "Use type hints for better code documentation",
                "Add docstrings to functions", 
                "Prefer method chaining over variable assignment",
                "Use functional patterns (map, filter, list comprehensions)"
            ],
            summary=f"Code analysis completed. Functional score: {functional_score}/100. {pure_functions} pure functions out of {total_functions} total functions. Quality level: {quality_level.value}",
            next_actions=immediate_fixes if immediate_fixes else ["Code quality is acceptable"]
        )
        
        return report
        
    except Exception as e:
        # Return error report
        return FunctionalPracticesReport(
            file_analyzed=file_path,
            overall_quality=QualityLevel.POOR,
            functional_score=0.0,
            code_metrics=CodeMetrics(
                total_lines=0,
                total_functions=0,
                average_function_length=0.0,
                cyclomatic_complexity_avg=0.0,
                pure_functions_count=0,
                impure_functions_count=0
            ),
            functional_issues=[],
            immutability_violations=[],
            side_effects_detected=[],
            pure_function_opportunities=[],
            immediate_fixes=[f"Analysis error: {str(e)}"],
            refactoring_suggestions=[],
            best_practices_recommendations=[],
            summary=f"Analysis failed: {str(e)}",
            next_actions=["Review the generated code manually"]
        )


if __name__ == "__main__":
    # Local test
    import asyncio
    
    async def test_analyzer():
        # Test with example file
        test_file = "generated_pipeline_example.py"
        
        try:
            report = analyze_code_quality_simple(test_file)
            log_analysis_results(report)
            print(f"Complete report: {report}")
            
        except Exception as e:
            print(f"Error in test: {e}")
    
    asyncio.run(test_analyzer())